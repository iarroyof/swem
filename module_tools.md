# The one-hot encoder
[scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)

[Numpy](http://stackoverflow.com/questions/38592324/one-hot-encoding-using-numpy)
## Creating dictionary
[Multiprocessing](http://stackoverflow.com/questions/8016561/how-can-i-parallelize-this-word-counting-function)

[Multiprocessing](https://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python)

[Multiprocessing](https://github.com/swarajban/multithreadedWordCounting)

Tal vez el problema es que dividen los archivos de datos en segmentos con igual número de líneas. Un segmento por nucleo de procesamiento. No todas las líneas son del mismo tamaño.
